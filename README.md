# Evaluating LLM Reasoning via Traditional Feature Importance

This repository contains code, data, and experiments for evaluating the quality of reasoning generated by large language models (LLMs) in text classification tasks. The evaluation framework leverages feature importance derived from traditional machine learning models (e.g., logistic regression, decision trees) as prior knowledge to assess the alignment and relevance of LLM-generated explanations.

